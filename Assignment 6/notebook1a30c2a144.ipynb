{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Silence noisy logs & install dependencies\nimport os\nimport warnings\nimport logging\n\n# 1) Silence TF/TPU logs\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nlogging.getLogger().setLevel(logging.ERROR)\n\n# 2) Suppress tqdm “IProgress not found” warning by matching its text\nwarnings.filterwarnings(\"ignore\", message=\"IProgress not found\")\n\n# 3) Install required packages\n!pip install --upgrade pip setuptools wheel\n!pip install --quiet transformers torch torchvision pillow scikit-learn pandas opencv-python-headless\n\nprint(\"✅ Cell 1 completed: dependencies installed and warnings silenced\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T02:25:24.708222Z","iopub.execute_input":"2025-05-17T02:25:24.708428Z","iopub.status.idle":"2025-05-17T02:26:56.701512Z","shell.execute_reply.started":"2025-05-17T02:25:24.708411Z","shell.execute_reply":"2025-05-17T02:26:56.700546Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\nCollecting setuptools\n  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\nDownloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: setuptools, pip\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pip-25.1.1 setuptools-80.7.1\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m162.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n\u001b[1A\u001b[2K✅ Cell 1 completed: dependencies installed and warnings silenced\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load & sample the Kaggle dataset (auto-detect folder)\n\nimport os, glob, random, pandas as pd\n\n# 1. Search /content and /content/data for a folder that has train/, validation/, test/\nBASE = None\nfor parent in ['/content','/kaggle/input/deepfake-and-real-images']:\n    if not os.path.isdir(parent): \n        continue\n    for d in os.listdir(parent):\n        p = os.path.join(parent, d)\n        if not os.path.isdir(p):\n            continue\n        subs = [name.lower() for name in os.listdir(p)]\n        if all(x in subs for x in ['train','validation','test']):\n            BASE = p\n            break\n    if BASE:\n        break\n\nif BASE is None:\n    raise FileNotFoundError(\n        \"Could not auto-detect the dataset folder under /content or /content/data. \"\n        f\"Found: {os.listdir('/content')} and {os.listdir('/content/data') if os.path.isdir('/content/data') else 'N/A'}\"\n    )\nprint(f\"▶️ Found dataset at: {BASE}\")\n\n# 2. Gather image paths from train/validation/test splits (real & fake)\npaths = []\nfor split in os.listdir(BASE):\n    if split.lower() not in ('train','validation','test'):\n        continue\n    split_dir = os.path.join(BASE, split)\n    for label in os.listdir(split_dir):\n        if label.lower() not in ('real','fake'):\n            continue\n        paths += glob.glob(os.path.join(split_dir, label, '*'))\n\nif not paths:\n    raise FileNotFoundError(f\"No images found under detected BASE: {BASE}\")\n\n# 3. Sample 100 images\nrandom.seed(42)\nsample = random.sample(paths, min(100, len(paths)))\ndf = pd.DataFrame({\n    'image': sample,\n    'true':  [os.path.basename(os.path.dirname(p)).upper() for p in sample]\n})\n\nprint(f\"✅ Cell 2 completed: loaded and sampled {len(df)} images from {BASE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T02:26:56.702674Z","iopub.execute_input":"2025-05-17T02:26:56.703405Z","iopub.status.idle":"2025-05-17T02:27:00.280534Z","shell.execute_reply.started":"2025-05-17T02:26:56.703374Z","shell.execute_reply":"2025-05-17T02:27:00.279870Z"}},"outputs":[{"name":"stdout","text":"▶️ Found dataset at: /kaggle/input/deepfake-and-real-images/Dataset\n✅ Cell 2 completed: loaded and sampled 100 images from /kaggle/input/deepfake-and-real-images/Dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: CPU-only inference, ensemble & binary metrics with 10 HF models\n\nimport os\n# Force CPU/PyTorch, disable TF/TPU\nos.environ['USE_TF'] = '0'\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\n\nfrom transformers import pipeline\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, precision_score, f1_score, accuracy_score\nimport pandas as pd\n\n# 10 Hugging Face deepfake-vs-real detectors\nMODEL_IDS = [\n    \"dima806/deepfake_vs_real_image_detection\",\n    \"prithivMLmods/Deep-Fake-Detector-Model\",\n    \"HrutikAdsare/deepfake-detector-faceforensics\",\n    \"prithivMLmods/AI-vs-Deepfake-vs-Real-v2.0\",\n    \"ashish-001/deepfake-detection-using-ViT\",\n    \"prithivMLmods/Deepfake-Detect-Siglip2\",\n    \"Wvolf/ViT_Deepfake_Detection\",\n    \"dima806/ai_vs_real_image_detection\",\n    \"joyc360/deepfakes\",\n    \"Skullly/DeepFake-image-detection-ViT-384\"\n]\n\n# Load pipelines (skip any that fail)\npipes = {}\nfor m in MODEL_IDS:\n    try:\n        pipes[m] = pipeline(\n            \"image-classification\",\n            model=m,\n            trust_remote_code=True,\n            framework=\"pt\",\n            device=-1\n        )\n        print(f\"✅ Loaded {m}\")\n    except Exception as e:\n        print(f\"⚠️ Skipped {m}: {type(e).__name__}\")\nassert pipes, \"No models loaded successfully.\"\n\n# Inference & majority-vote ensemble\npreds = []\nfor img in df['image']:\n    votes = []\n    for p in pipes.values():\n        try:\n            votes.append(p(img)[0]['label'].upper())\n        except:\n            pass\n    preds.append(Counter(votes).most_common(1)[0][0] if votes else 'REAL')\ndf['pred'] = preds\n\n# Convert to binary 1=REAL, 0=FAKE\ny_true = [1 if t == 'REAL' else 0 for t in df['true']]\ny_pred = [1 if p == 'REAL' else 0 for p in df['pred']]\n\n# Compute metrics\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\nmetrics = {\n    'Sensitivity (Recall)': tp/(tp+fn) if (tp+fn) else 0,\n    'Specificity':           tn/(tn+fp) if (tn+fp) else 0,\n    'Precision':             precision_score(y_true, y_pred, zero_division=0),\n    'F1-Score':              f1_score(y_true, y_pred, zero_division=0),\n    'Accuracy':              accuracy_score(y_true, y_pred)\n}\n\n# Display results\nprint(\"\\n## Ensemble Results (100-sample test)\\n\")\nprint(pd.DataFrame.from_dict(metrics, orient='index', columns=['Value']).to_markdown())\nprint(\"\\n✅ Cell 3 completed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T02:33:34.721885Z","iopub.execute_input":"2025-05-17T02:33:34.722221Z","iopub.status.idle":"2025-05-17T02:39:02.071167Z","shell.execute_reply.started":"2025-05-17T02:33:34.722195Z","shell.execute_reply":"2025-05-17T02:39:02.070269Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded dima806/deepfake_vs_real_image_detection\n✅ Loaded prithivMLmods/Deep-Fake-Detector-Model\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded HrutikAdsare/deepfake-detector-faceforensics\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded prithivMLmods/AI-vs-Deepfake-vs-Real-v2.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e5a056f4c05444e9d98f9d23244ee80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"134cea017826417ab7d75ae47118f8e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc875198bfb45f9ba1a0f891020a5df"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded ashish-001/deepfake-detection-using-ViT\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a1a847bc734337ac8da8d07cfc111e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/372M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6348da2bc774282bea2d31aa4345705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be5a4fe06a53421d8ac1f9b1220f0264"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded prithivMLmods/Deepfake-Detect-Siglip2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb42d8b0ff74ff68c3893c1e5db2fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7091f0f375744bde921286291d25ffeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/325 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc1e1276ade43c8ba0177b58b00021b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded Wvolf/ViT_Deepfake_Detection\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"987b37551c8e48d894900fb6094e3e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a3fe44343d49b79b2cf7c8e58f6b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e14920a7e74ba6b17e4dc76090dc2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092c3c7509074df7b1f54b81a2b2f4eb"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded dima806/ai_vs_real_image_detection\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2967b2feaf1402a853ad736241028ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccbc2aad7974b84b5d7f9841c90fb17"}},"metadata":{}},{"name":"stderr","text":"Invalid model-index. Not loading eval results into CardData.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/327 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a26271e51442ba919c84e2ca87f4dc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded joyc360/deepfakes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadd3bf659814fa49958980ac4e8e544"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cde5353f774c2b8310e50473586f5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9dadd666e04d33b41d848d8f4b5f73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7efd39ac084519b2f055ae023688c5"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded Skullly/DeepFake-image-detection-ViT-384\n\n## Ensemble Results (100-sample test)\n\n|                      |    Value |\n|:---------------------|---------:|\n| Sensitivity (Recall) | 1        |\n| Specificity          | 0.716981 |\n| Precision            | 0.758065 |\n| F1-Score             | 0.862385 |\n| Accuracy             | 0.85     |\n\n✅ Cell 3 completed\n","output_type":"stream"}],"execution_count":5}]}